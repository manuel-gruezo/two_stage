# PRTR Two-stage Cascade Transformers

## Informaci√≥n del Art√≠culo Base

**Nombre del art√≠culo:** Pose Recognition with Cascade Transformers

**Enlace al art√≠culo:** [Pose Recognition with Cascade Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Pose_Recognition_With_Cascade_Transformers_CVPR_2021_paper.pdf)

**Repositorio original:** [PRTR en GitHub](https://github.com/mlpc-ucsd/PRTR/tree/main)

## Descripci√≥n 

Esta aplicaci√≥n ofrece una interfaz web hecha con Streamlit para realizar detecci√≥n de poses humanas en im√°genes y videos utilizando el modelo PRTR (Pose Recognition with Cascade Transformers). Esta t√©cnica detecta personas y estima sus 17 keypoints corporales mediante una arquitectura de Transformers en cascada, permitiendo identificar y analizar poses con alta precisi√≥n y eficiencia. A diferencia de los enfoques tradicionales basados en heatmaps, PRTR adopta un m√©todo de regresi√≥n directa que elimina la necesidad de complejos pre-procesamientos y post-procesamientos heur√≠sticos.

Adem√°s de la detecci√≥n de poses, la aplicaci√≥n incluye una funcionalidad avanzada de **conteo autom√°tico de sentadillas** que analiza videos de ejercicios en tiempo real. Esta caracter√≠stica detecta autom√°ticamente cuando una persona realiza una sentadilla completa bas√°ndose en el an√°lisis de los keypoints corporales (caderas y rodillas), proporcionando un conteo preciso y visualizaci√≥n del estado del movimiento (BAJANDO/SUBIENDO) superpuesto en el video procesado.


## Interfaz

La aplicaci√≥n ofrece una interfaz web intuitiva y moderna construida con Streamlit. A continuaci√≥n se muestran capturas de pantalla de las diferentes secciones:

### P√°gina de Inicio

![P√°gina de inicio de la aplicaci√≥n](figures/pagina_inicio.png)

La p√°gina de inicio presenta un dise√±o limpio y profesional con:
- **Header**: T√≠tulo "Pose Recognition" 
- **Sidebar de configuraci√≥n**: Panel lateral con opciones de dise√±o visual y ajustes de par√°metros
- **Modos de an√°lisis**: Selecci√≥n entre diferentes modos (An√°lisis de Imagen, Tomar Foto, An√°lisis de Video, Conteo de sentadillas)
- **Descripci√≥n de caracter√≠sticas**: Informaci√≥n detallada sobre cada funcionalidad disponible

### Resultados de Detecci√≥n

![Resultados de detecci√≥n de poses](figures/predict2.png)

La secci√≥n de resultados muestra:
- **Comparaci√≥n lado a lado**: Visualizaci√≥n de la imagen original junto con los resultados de detecci√≥n
- **Opci√≥n de descarga**: Bot√≥n para descargar los resultados procesados



## Ejemplos

### Contador de Sentadillas

<div align="center">
  <table>
    <tr>
      <td align="center">
        <img src="figures/1.gif" alt="Ejemplo 1 - Contador de sentadillas" width="100%"/>
      </td>
      <td align="center">
        <img src="figures/2.gif" alt="Ejemplo 2 - Contador de sentadillas" width="100%"/>
      </td>
      <td align="center">
        <img src="figures/3.gif" alt="Ejemplo 3 - Contador de sentadillas" width="100%"/>
      </td>
    </tr>
  </table>
</div>

### Innovaciones principales:

**Arquitectura Two-Stage en Cascada:**
- **Person-Detection Transformer (DETR)**: Detecta personas en la imagen completa usando el mecanismo de matching bipartito de DETR. DETR usa matching bipartito que evita duplicados.
- **Keypoint-Detection Transformer (PRTR)**: Predice los 17 keypoints COCO para cada persona detectada mediante regresi√≥n directa de coordenadas, haciendo el pipeline completamente diferenciable.
- **Mecanismo de Queries**: 100 queries aprendidas que compiten para predecir los 17 keypoints, asignadas √≥ptimamente mediante el algoritmo h√∫ngaro basado en probabilidad de clase.
- **Algoritmo H√∫ngaro para Entrenamiento**: Durante el entrenamiento, el algoritmo h√∫ngaro asigna √≥ptimamente las queries a los keypoints ground truth, permitiendo que el modelo aprenda a refinar progresivamente las predicciones a trav√©s de las capas del decoder. Este matching √≥ptimo asegura que cada query se especialice en predecir un keypoint espec√≠fico, facilitando el refinamiento iterativo de las predicciones.

## Resumen Te√≥rico de la Arquitectura

### Arquitectura Two-Stage

![model_two_stage](https://raw.githubusercontent.com/mlpc-ucsd/PRTR/refs/heads/main/figures/model_two_stage.png)

La arquitectura two-stage de PRTR consiste en:

#### 1. Person-Detection Transformer (DETR)

- **Backbone CNN**: Extrae caracter√≠sticas de la imagen completa
- **Transformer Encoder-Decoder**: Procesa las caracter√≠sticas con atenci√≥n
- **Salida**: Detecta personas con bounding boxes usando el mecanismo de matching bipartito de DETR
- **Filtrado**: DETR usa matching bipartito que evita duplicados, por lo que no requiere NMS. Se filtra por umbral de confianza (threshold) y se ordenan las detecciones por score descendente.

#### 2. Keypoint-Detection Transformer (PRTR)

- **Input**: Recortes de personas detectadas (expandidos 12.5% en cada direcci√≥n, 25% total para contexto adicional)
- **Procesamiento**: Cada recorte se procesa independientemente con transformaci√≥n af√≠n al tama√±o del modelo (512x384 p√≠xeles, height x width)
- **Mecanismo de Queries**: 100 queries aprendidas que compiten para predecir los 17 keypoints
- **Hungarian Matching**: Asigna √≥ptimamente queries a keypoints usando el algoritmo de asignaci√≥n lineal (linear_sum_assignment) basado en las probabilidades de clase. Este matching se realiza internamente en `get_final_preds_match()`.
- **Filtrado Posterior**: Despu√©s del Hungarian Matching, se aplican filtros adicionales para validar keypoints:
  - Probabilidad de clase > 30%
  - Distancia espacial < 50 p√≠xeles
  - Confianza del heatmap > 80%
- **Salida**: maximo de  17 keypoints por persona

### Proceso de Inferencia

1. **Detecci√≥n**: El Person-Detection Transformer identifica todas las personas en la imagen
2. **Recorte**: Cada detecci√≥n se expande 12.5% en cada direcci√≥n (25% total) y se recorta
3. **Normalizaci√≥n**: Transformaci√≥n af√≠n al tama√±o del modelo (512x384 p√≠xeles, width x height) usando centro y escala calculados del recorte
4. **Predicci√≥n**: Keypoint-Detection Transformer procesa cada recorte
5. **Flip-Test**: Promedio de predicciones con imagen espejo para mayor robustez
6. **Transformaci√≥n Inversa**: Coordenadas se mapean de vuelta al espacio original:
   - Espacio del modelo ‚Üí Espacio del recorte (transformaci√≥n af√≠n inversa)
   - Espacio del recorte ‚Üí Imagen original (aplicaci√≥n de offsets de bounding box)

## Pasos para ejecutar el proyecto

### Prerrequisitos

- Docker y Docker Compose instalados
- CUDA (opcional, para aceleraci√≥n GPU)
- 8GB+ RAM recomendado

### Estructura de Modelos

Asegurar la siguiente estructura de archivos en el directorio del proyecto:

```
models/
‚îú‚îÄ‚îÄ pytorch/
‚îÇ   ‚îî‚îÄ‚îÄ pose_coco/
‚îÇ       ‚îî‚îÄ‚îÄ pose_transformer_hrnet_w32_512x384.pth
‚îî‚îÄ‚îÄ detr-resnet-101/
    ‚îú‚îÄ‚îÄ config.json
    ‚îú‚îÄ‚îÄ model.safetensors
    ‚îî‚îÄ‚îÄ preprocessor_config.json
```
Descargar en el siguiente link [link models](https://drive.google.com/drive/folders/1eJCNwf9BnJf0YoOLLk5rKQw7pdFtb33w?usp=sharing)

### 1) Despliegue con Docker

#### Construcci√≥n y Ejecuci√≥n

```bash
# Construir imagen
docker compose build

# Ejecutar contenedor
docker compose up
```

#### Acceso a la Aplicaci√≥n

- Abrir en el navegador: `http://localhost:8501`
- Para detener el servicio: `Ctrl + C`

### 2) Uso de la Aplicaci√≥n Streamlit

#### Modo "An√°lisis de Imagen"
- Subir una imagen (formatos: JPG, JPEG, PNG, BMP)
- Ajustar par√°metros en la barra lateral:
  - **Sensibilidad de detecci√≥n**: Controla umbral de confianza DETR (0.9 recomendado)
  - **Tama√±o de puntos**: Ajusta visualizaci√≥n de keypoints
  - **Grosor de l√≠neas**: Controla grosor del esqueleto visible
- Presionar "Analizar Pose"

#### Modo "Tomar Foto"
- Permitir acceso a c√°mara
- Capturar foto
- Presionar "Analizar Pose"

#### Modo "An√°lisis de Video"
- Subir un video (formatos: MP4, AVI, MOV, MKV, WEBM)
- Configurar opciones de procesamiento:
  - **Frame skip**: Procesar cada N frames (1-10, recomendado: 2)
  - **Orientaci√≥n**: Horizontal o Vertical
  - **Resoluci√≥n**: 360p, 480p, 720p, 1080p u Original
  - **L√≠mite de frames**: Opcional para videos largos
- Presionar "Procesar Video"
- Descargar video procesado con poses detectadas

#### Modo "Conteo de sentadillas"
- Subir un video con ejercicios de sentadillas
- Configurar opciones de procesamiento (similar a An√°lisis de Video)
- Presionar "Procesar Video"
- El sistema cuenta autom√°ticamente las sentadillas realizadas
- Visualizaci√≥n en tiempo real del conteo y estado (BAJANDO/SUBIENDO)
- Descargar video procesado con conteo superpuesto

#### Resultados
- Comparaci√≥n lado a lado: Imagen original vs Imagen con poses detectadas
- M√©tricas de detecci√≥n: N√∫mero de personas detectadas
- Tiempo de inferencia: Tiempo de procesamiento
- Opci√≥n de descarga de resultados en formato PNG

## ¬øC√≥mo se cargan los pesos?

La funci√≥n `load_models()` en `app.py`:

1. **Modelo de Pose (PRTR)**:
   - Lee la configuraci√≥n desde `experiments/coco/transformer/w32_512x384_adamw_lr1e-4.yaml`
   - Carga la arquitectura PRTR-HRNet W32
   - Inyecta los pesos preentrenados desde `models/pytorch/pose_coco/pose_transformer_hrnet_w32_512x384.pth`
   - Pone el modelo en modo evaluaci√≥n (`eval()`)

2. **Modelo DETR (Detecci√≥n)**:
   - Carga el procesador y modelo DETR desde la carpeta local `models/detr-resnet-101/`
   - Usa `DetrImageProcessor` para preprocesamiento
   - Usa `DetrForObjectDetection` para inferencia

3. **Dispositivo**:
   - Selecciona autom√°ticamente `cuda` si est√° disponible, en caso contrario `cpu`

**Rutas importantes (por defecto en `app.py`)**:
- Pose: `models/pytorch/pose_coco/pose_transformer_hrnet_w32_512x384.pth`
- DETR: `models/detr-resnet-101/` (directorio con los ficheros del modelo)

**Nota**: Si cambias la ubicaci√≥n de los modelos, ajusta en `app.py`:
- Ruta del pose: atributo `pretrained` de la clase `Args`
- Ruta del DETR: variable `local_model_path` dentro de `load_models()`

## ¬øC√≥mo se realiza la inferencia?

### Pipeline de Inferencia

#### Fase 1: Detecci√≥n de Personas

1. **Procesamiento DETR**:
   - Imagen completa ‚Üí caracter√≠sticas extra√≠das
   - Post-procesamiento: filtrado por clase "persona" (label == 1 en COCO)
   - Umbral de confianza configurable (0.9 recomendado)
   - DETR usa matching bipartito que evita duplicados, por lo que no requiere NMS
   - Las detecciones se ordenan por score descendente

2. **Expansi√≥n de Bounding Boxes**:
   - Cada caja se expande 12.5% en cada direcci√≥n (25% total)
   - Proporciona contexto adicional para keypoints cerca de los bordes
   - Mejora la detecci√≥n de extremidades

#### Fase 2: Estimaci√≥n de Pose

1. **Transformaci√≥n Af√≠n**:
   - **Centro**: `(w/2, h/2)` del recorte
   - **Escala**: `max(w, h) / 200.0 * 1.25` (factor 1.25 para margen)
   - **Tama√±o objetivo**: 512x384 p√≠xeles (width x height)

2. **Inferencia del Modelo**:
   - **Forward pass original**: Recorte transformado ‚Üí predicci√≥n
   - **Forward pass flipped**: Imagen espejo ‚Üí predicci√≥n
   - **Promedio**: `(predicci√≥n_original + predicci√≥n_flipped) / 2.0`
   - Mayor robustez ante variaciones de orientaci√≥n

3. **Hungarian Matching**:
   - Se ejecuta internamente en `get_final_preds_match()` usando `linear_sum_assignment`
   - 100 queries aprendidas compiten para predecir 17 keypoints
   - Asignaci√≥n √≥ptima basada en las probabilidades de clase (matriz de costos negativa)
   - El algoritmo encuentra la mejor asignaci√≥n 1-a-1 entre queries y keypoints

4. **Filtrado Posterior**:
   - Despu√©s del Hungarian Matching, se aplican filtros adicionales para validar keypoints:
     - **Probabilidad de clase** > 30%
     - **Distancia espacial** < 50 p√≠xeles
     - **Confianza del heatmap** > 80%
   - Keypoints con baja confianza se enmascaran como NaN

5. **Transformaci√≥n Inversa**:
   - **Paso 1**: Coordenadas modelo ‚Üí espacio recorte (transformaci√≥n af√≠n inversa)
   - **Paso 2**: Espacio recorte ‚Üí imagen original (aplicaci√≥n de offsets de bounding box)
   - Garantiza que los keypoints se dibujen en la posici√≥n correcta

#### Fase 3: Visualizaci√≥n

1. **Dibujado del Esqueleto**:
   - Conexi√≥n de pares de joints seg√∫n definici√≥n COCO
   - L√≠neas coloridas por segmento corporal:
     - Verde: brazo izquierdo
     - Amarillo: brazo derecho
     - Azul: pierna izquierda
     - Rosa: pierna derecha
     - Rosa claro: conexiones de cabeza

2. **M√©tricas y Estad√≠sticas**:
   - N√∫mero de personas detectadas
   - Tiempo de inferencia del procesamiento
   - Visualizaci√≥n de poses con keypoints y esqueleto dibujados

### Configuraciones Clave

#### Par√°metros DETR
- **Umbral confianza**: 0.9 (balance precisi√≥n/recall)
- **Matching bipartito**: DETR evita duplicados autom√°ticamente
- **Expansi√≥n bbox**: 12.5% en cada direcci√≥n (25% total) para mejor contexto de keypoints

#### Par√°metros PRTR
- **Tama√±o entrada**: 512x384 p√≠xeles (width x height, √≥ptimo para HRNet-W32)
- **Flip-test**: Habilitado (mejora robustez)
- **Umbral keypoints**: 0.8 (filtrado conservador)
- **Hungarian Matching**: Autom√°tico (basado en probabilidades de clase)
- **Filtrado posterior**: Probabilidad > 30%, distancia < 50px, confianza > 80%

## ¬øC√≥mo funciona el conteo de sentadillas?

El sistema de conteo autom√°tico de sentadillas utiliza los keypoints detectados por el modelo PRTR para analizar el movimiento de la persona y contar las sentadillas completas realizadas.

### Algoritmo de Detecci√≥n

El algoritmo se basa en el an√°lisis de la posici√≥n relativa entre las **caderas** y las **rodillas** de la persona:

1. **Keypoints Utilizados**:
   - **Caderas**: Keypoints 11 (cadera izquierda) y 12 (cadera derecha) del formato COCO
   - **Rodillas**: Keypoints 13 (rodilla izquierda) y 14 (rodilla derecha) del formato COCO

2. **C√°lculo de Posici√≥n**:
   - Se calcula el promedio de las coordenadas Y (vertical) de ambas caderas
   - Se calcula el promedio de las coordenadas Y de ambas rodillas
   - Se calcula la diferencia: `diff = hip_y_avg - knee_y_avg`
   - En el sistema de coordenadas, Y aumenta hacia abajo

3. **Detecci√≥n de Estado**:
   - **Posici√≥n de pie (UP)**: Cuando la cadera est√° significativamente arriba de la rodilla (`diff < -50 p√≠xeles`)
   - **Posici√≥n de sentadilla (DOWN)**: Cuando la cadera est√° cerca o por debajo de la rodilla (`diff > -50 p√≠xeles`)

4. **Conteo de Sentadillas**:
   - El sistema mantiene un estado por persona (`'up'` o `'down'`)
   - Se detecta una **transici√≥n de arriba a abajo** cuando cambia de `'up'` a `'down'` (persona empieza a bajar)
   - Se detecta una **transici√≥n de abajo a arriba** cuando cambia de `'down'` a `'up'` (persona completa la sentadilla)
   - **Una sentadilla completa** se cuenta cuando se detecta la transici√≥n de `'down'` a `'up'`

### Procesamiento de Video

Para videos, el sistema:

1. **Procesa cada frame** (o cada N frames seg√∫n el frame skip configurado)
2. **Detecta personas** usando DETR y estima sus keypoints con PRTR
3. **Mantiene estado independiente** para cada persona detectada
4. **Actualiza el conteo** en tiempo real seg√∫n las transiciones detectadas
5. **Visualiza el resultado** superponiendo en el video:
   - N√∫mero total de sentadillas contadas
   - Estado actual: "BAJANDO" (rojo) o "SUBIENDO" (verde)

### Caracter√≠sticas del Algoritmo

- **Robustez**: Solo cuenta sentadillas cuando hay suficientes keypoints visibles (al menos una cadera y una rodilla)
- **Multi-persona**: Mantiene conteos independientes para cada persona en el video
- **Tolerancia**: Usa un umbral de 50 p√≠xeles para evitar falsos positivos por peque√±os movimientos
- **Visualizaci√≥n en tiempo real**: Muestra el conteo y estado durante el procesamiento del video

### Limitaciones

- Requiere que la persona est√© completamente visible en el video
- Funciona mejor con vista lateral o frontal de la persona
- La precisi√≥n depende de la calidad de detecci√≥n de keypoints (iluminaci√≥n, fondo, etc.)
- Puede tener dificultades con movimientos muy r√°pidos si el frame skip es muy alto

## Agradecimientos
Este proyecto est√° basado en los siguientes repositorios de c√≥digo abierto, que facilitan enormemente nuestra investigaci√≥n.

- Gracias a [DETR](https://github.com/facebookresearch/detr) por la implementaci√≥n de [Detection Transformer](https://arxiv.org/abs/2005.12872)

- Gracias a [PRTR](https://github.com/mlpc-ucsd/PRTR/tree/main) por la implementaci√≥n de [Pose Recognition with Cascade Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Pose_Recognition_With_Cascade_Transformers_CVPR_2021_paper.pdf)


## Herramientas Utilizadas

- üß∞ **[Git](https://git-scm.com/)**
- üóÇ **[GitHub](https://github.com/)**
- üêç **[Python](https://www.python.org/)**
- üî• **[Pytorch](https://pytorch.org/)**
- üê≥ **[Docker](https://www.docker.com/)**
- üìà **[Streamlit](https://streamlit.io/)**
- üß¨ **[Hugging Face Transformers](https://huggingface.co/docs/transformers)**

## Autores

- **Jhonatan Steven Morales / jhonatan19991**  
- **Manuel Alejandro Perlaza / manuel-gruezo**  
- **Carol Dayana Varela / caroldvarela**  