# PRTR Two-stage Cascade Transformers

## InformaciÃ³n del ArtÃ­culo Base

**Nombre del artÃ­culo:** Pose Recognition with Cascade Transformers

**Enlace al artÃ­culo:** [Pose Recognition with Cascade Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Pose_Recognition_With_Cascade_Transformers_CVPR_2021_paper.pdf)

**Repositorio original:** [PRTR en GitHub](https://github.com/mlpc-ucsd/PRTR/tree/main)

## DescripciÃ³n 

Esta aplicaciÃ³n ofrece una interfaz web (Streamlit) para realizar detecciÃ³n de poses humanas en imÃ¡genes utilizando el modelo PRTR (Pose Recognition with Cascade Transformers). Esta tÃ©cnica detecta personas y estima sus 17 keypoints corporales mediante una arquitectura de Transformers en cascada, permitiendo identificar y analizar poses con alta precisiÃ³n y eficiencia. A diferencia de los enfoques tradicionales basados en heatmaps, PRTR adopta un mÃ©todo de regresiÃ³n directa que elimina la necesidad de complejos pre-procesamientos y post-procesamientos heurÃ­sticos.


## Interfaz



## Ejemplos






### Innovaciones principales:

**Arquitectura Two-Stage en Cascada:**
- **Person-Detection Transformer (DETR)**: Detecta personas en la imagen completa usando el mecanismo de matching bipartito de DETR. Aunque DETR puede generar detecciones duplicadas, se aplica NMS (Non-Maximum Suppression) para eliminar duplicados y evitar detectar la misma persona mÃºltiples veces.
- **Keypoint-Detection Transformer (PRTR)**: Predice los 17 keypoints COCO para cada persona detectada mediante regresiÃ³n directa de coordenadas, haciendo el pipeline completamente diferenciable. Este modelo NO requiere NMS ya que usa Hungarian Matching para asignar queries a keypoints.
- **Mecanismo de Queries**: 100 queries aprendidas que compiten para predecir los 17 keypoints, asignadas Ã³ptimamente mediante Hungarian Matching basado en probabilidad de clase.
- **Refinamiento Gradual**: Las queries de keypoints se refinan progresivamente a travÃ©s de las capas del decoder, mejorando iterativamente las predicciones.

## Resumen TeÃ³rico de la Arquitectura

### Arquitectura Two-Stage

![model_two_stage](https://raw.githubusercontent.com/mlpc-ucsd/PRTR/refs/heads/main/figures/model_two_stage.png)

La arquitectura two-stage de PRTR consiste en:

#### 1. Person-Detection Transformer (DETR)

- **Backbone CNN**: Extrae caracterÃ­sticas de la imagen completa
- **Transformer Encoder-Decoder**: Procesa las caracterÃ­sticas con atenciÃ³n
- **Salida**: Detecta personas con bounding boxes usando el mecanismo de matching bipartito de DETR
- **NMS**: Aunque DETR usa matching bipartito, puede generar detecciones duplicadas. Por lo tanto, se aplica NMS (Non-Maximum Suppression) para eliminar duplicados y evitar detectar la misma persona mÃºltiples veces.

#### 2. Keypoint-Detection Transformer (PRTR)

- **Input**: Recortes de personas detectadas (expandidos 25% para contexto adicional)
- **Procesamiento**: Cada recorte se procesa independientemente con transformaciÃ³n afÃ­n al tamaÃ±o del modelo (512x384 pÃ­xeles, width x height)
- **Mecanismo de Queries**: 100 queries aprendidas que compiten para predecir los 17 keypoints
- **Hungarian Matching**: Asigna Ã³ptimamente queries a keypoints usando el algoritmo de asignaciÃ³n lineal (linear_sum_assignment) basado en las probabilidades de clase. Este matching se realiza internamente en `get_final_preds_match()`.
- **Filtrado Posterior**: DespuÃ©s del Hungarian Matching, se aplican filtros adicionales para validar keypoints:
  - Probabilidad de clase > 30%
  - Distancia espacial < 50 pÃ­xeles
  - Confianza del heatmap > 80%
- **Salida**: 17 keypoints en coordenadas absolutas con sus confianzas

### Proceso de Inferencia

1. **DetecciÃ³n**: El Person-Detection Transformer identifica todas las personas en la imagen
2. **Recorte**: Cada detecciÃ³n se expande un 25% y se recorta
3. **NormalizaciÃ³n**: TransformaciÃ³n afÃ­n al tamaÃ±o del modelo (512x384 pÃ­xeles, width x height) usando centro y escala calculados del recorte
4. **PredicciÃ³n**: Keypoint-Detection Transformer procesa cada recorte
5. **Flip-Test**: Promedio de predicciones con imagen espejo para mayor robustez
6. **TransformaciÃ³n Inversa**: Coordenadas se mapean de vuelta al espacio original:
   - Espacio del modelo â†’ Espacio del recorte (transformaciÃ³n afÃ­n inversa)
   - Espacio del recorte â†’ Imagen original (aplicaciÃ³n de offsets de bounding box)

## Pasos para ejecutar el proyecto

### Prerrequisitos

- Python 3.9+
- CUDA (opcional, para aceleraciÃ³n GPU)
- 8GB+ RAM recomendado

### 1) EjecuciÃ³n Local

#### ConfiguraciÃ³n del Entorno

```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno (Windows)
venv\Scripts\activate

# Activar entorno (Linux/Mac)
source venv/bin/activate

# Instalar dependencias
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
```

#### Estructura de Modelos

Asegurar la siguiente estructura de archivos:

```
models/
â”œâ”€â”€ pytorch/
â”‚   â””â”€â”€ pose_coco/
â”‚       â””â”€â”€ pose_transformer_hrnet_w32_512x384.pth
â””â”€â”€ detr-resnet-101/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors
    â””â”€â”€ preprocessor_config.json
```
Descargar en el siguinte link [link models](https://drive.google.com/drive/folders/1eJCNwf9BnJf0YoOLLk5rKQw7pdFtb33w?usp=sharing)

#### EjecuciÃ³n por Lotes

El script `testInference.py` procesa todas las imÃ¡genes del directorio `local/input/` y guarda los resultados en `local/output/`.

```bash
# Procesar todas las imÃ¡genes en local/input/
python local/testInference.py
```

**Funcionamiento:**
- Lee automÃ¡ticamente todas las imÃ¡genes del directorio `local/input/` (formatos: `.jpg`, `.jpeg`, `.png`, `.bmp`)
- Aplica la misma lÃ³gica de inferencia que `app.py` (DETR + NMS + PRTR)
- Procesa cada imagen y detecta personas con sus keypoints
- Guarda los resultados en `local/output/`:
  - **ImÃ¡genes**: `{nombre_imagen}_pose.jpg` - ImÃ¡genes con poses detectadas dibujadas
  - **JSON**: `{nombre_imagen}_detections.json` - Metadatos detallados con coordenadas, confianzas y bounding boxes por persona

**ParÃ¡metros de configuraciÃ³n** (definidos en el script):
- `DETR_THRESHOLD = 0.9` - Umbral de confianza para detecciÃ³n DETR
- `NMS_THRESHOLD = 0.5` - Umbral IoU para eliminar detecciones duplicadas
- `POINT_SIZE_MULTIPLIER = 1.0` - TamaÃ±o de los puntos de keypoints
- `LINE_WIDTH_MULTIPLIER = 1.0` - Grosor de las lÃ­neas del esqueleto

### 2) Despliegue con Docker

#### ConstrucciÃ³n y EjecuciÃ³n

```bash
# Construir imagen
docker compose build

# Ejecutar contenedor
docker compose up
```

#### Acceso a la AplicaciÃ³n

- Abrir en el navegador: `http://localhost:8501`
- Para detener el servicio: `Ctrl + C`

### 3) Uso de la AplicaciÃ³n Streamlit

#### Modo "AnÃ¡lisis de Imagen"
- Subir una imagen (formatos: JPG, JPEG, PNG, BMP)
- Ajustar parÃ¡metros en la barra lateral:
  - **Umbral DETR**: Controla sensibilidad de detecciÃ³n (0.9 recomendado)
  - **Umbral NMS**: Elimina duplicados (0.5 recomendado)
  - **TamaÃ±o puntos**: Ajusta visualizaciÃ³n de keypoints
  - **Grosor lÃ­neas**: Controla esqueleto visible
- Presionar "Analizar Pose"

#### Modo "Tomar Foto"
- Permitir acceso a cÃ¡mara
- Capturar foto
- Presionar "Analizar Pose"

#### Resultados
- Imagen con poses superpuestas
- Tabla de keypoints por persona
- Coordenadas y confianzas detalladas
- OpciÃ³n de descarga de resultados

## Â¿CÃ³mo se cargan los pesos?

La funciÃ³n `load_models()` en `app.py`:

1. **Modelo de Pose (PRTR)**:
   - Lee la configuraciÃ³n desde `experiments/coco/transformer/w32_512x384_adamw_lr1e-4.yaml`
   - Carga la arquitectura PRTR-HRNet W32
   - Inyecta los pesos preentrenados desde `models/pytorch/pose_coco/pose_transformer_hrnet_w32_512x384.pth`
   - Pone el modelo en modo evaluaciÃ³n (`eval()`)

2. **Modelo DETR (DetecciÃ³n)**:
   - Carga el procesador y modelo DETR desde la carpeta local `models/detr-resnet-101/`
   - Usa `DetrImageProcessor` para preprocesamiento
   - Usa `DetrForObjectDetection` para inferencia

3. **Dispositivo**:
   - Selecciona automÃ¡ticamente `cuda` si estÃ¡ disponible, en caso contrario `cpu`

**Rutas importantes (por defecto en `app.py`)**:
- Pose: `models/pytorch/pose_coco/pose_transformer_hrnet_w32_512x384.pth`
- DETR: `models/detr-resnet-101/` (directorio con los ficheros del modelo)

**Nota**: Si cambias la ubicaciÃ³n de los modelos, ajusta en `app.py`:
- Ruta del pose: atributo `pretrained` de la clase `Args`
- Ruta del DETR: variable `local_model_path` dentro de `load_models()`

## Â¿CÃ³mo se realiza la inferencia?

### Pipeline de Inferencia

#### Fase 1: DetecciÃ³n de Personas

1. **Procesamiento DETR**:
   - Imagen completa â†’ caracterÃ­sticas extraÃ­das
   - Post-procesamiento: filtrado por clase "persona" (label == 1 en COCO)
   - Umbral de confianza configurable (0.9 recomendado)

2. **NMS (Non-Maximum Suppression)**:
   - **Nota importante**: Aunque DETR usa matching bipartito, puede generar detecciones duplicadas de la misma persona
   - Se aplica NMS para eliminar estas detecciones duplicadas
   - CÃ¡lculo de IoU (Intersection over Union) entre cajas
   - Umbral IoU configurable (0.5 recomendado)
   - Esto asegura que cada persona sea detectada solo una vez

3. **ExpansiÃ³n de Bounding Boxes**:
   - Cada caja se expande 25% en cada direcciÃ³n (50% total)
   - Proporciona contexto adicional para keypoints cerca de los bordes
   - Mejora la detecciÃ³n de extremidades

#### Fase 2: EstimaciÃ³n de Pose

1. **TransformaciÃ³n AfÃ­n**:
   - **Centro**: `(w/2, h/2)` del recorte
   - **Escala**: `max(w, h) / 200.0 * 1.25` (factor 1.25 para margen)
   - **TamaÃ±o objetivo**: 512x384 pÃ­xeles (width x height)

2. **Inferencia del Modelo**:
   - **Forward pass original**: Recorte transformado â†’ predicciÃ³n
   - **Forward pass flipped**: Imagen espejo â†’ predicciÃ³n
   - **Promedio**: `(predicciÃ³n_original + predicciÃ³n_flipped) / 2.0`
   - Mayor robustez ante variaciones de orientaciÃ³n

3. **Hungarian Matching**:
   - Se ejecuta internamente en `get_final_preds_match()` usando `linear_sum_assignment`
   - 100 queries aprendidas compiten para predecir 17 keypoints
   - AsignaciÃ³n Ã³ptima basada en las probabilidades de clase (matriz de costos negativa)
   - El algoritmo encuentra la mejor asignaciÃ³n 1-a-1 entre queries y keypoints

4. **Filtrado Posterior**:
   - DespuÃ©s del Hungarian Matching, se aplican filtros adicionales para validar keypoints:
     - **Probabilidad de clase** > 30%
     - **Distancia espacial** < 50 pÃ­xeles
     - **Confianza del heatmap** > 80%
   - Si no se cumplen condiciones estrictas, se buscan queries de soporte cercanas
   - Keypoints con confianza > 0.8 se aceptan
   - Keypoints con baja confianza se enmascaran como NaN

5. **TransformaciÃ³n Inversa**:
   - **Paso 1**: Coordenadas modelo â†’ espacio recorte (transformaciÃ³n afÃ­n inversa)
   - **Paso 2**: Espacio recorte â†’ imagen original (aplicaciÃ³n de offsets de bounding box)
   - Garantiza que los keypoints se dibujen en la posiciÃ³n correcta

#### Fase 3: VisualizaciÃ³n

1. **Dibujado del Esqueleto**:
   - ConexiÃ³n de pares de joints segÃºn definiciÃ³n COCO
   - LÃ­neas coloridas por segmento corporal:
     - Verde: brazo izquierdo
     - Amarillo: brazo derecho
     - Azul: pierna izquierda
     - Rosa: pierna derecha
     - Rosa claro: conexiones de cabeza

2. **Tablas de Resultados**:
   - Detalle por persona con coordenadas (x, y) y confianza para cada keypoint
   - InformaciÃ³n de bounding box usado para el recorte

### Configuraciones Clave

#### ParÃ¡metros DETR
- **Umbral confianza**: 0.9 (balance precisiÃ³n/recall)
- **NMS threshold**: 0.5 (eliminaciÃ³n duplicados)
- **ExpansiÃ³n bbox**: 25% (mejor contexto para keypoints)

#### ParÃ¡metros PRTR
- **TamaÃ±o entrada**: 512x384 pÃ­xeles (width x height, Ã³ptimo para HRNet-W32)
- **Flip-test**: Habilitado (mejora robustez)
- **Umbral keypoints**: 0.8 (filtrado conservador)
- **Hungarian Matching**: AutomÃ¡tico (basado en probabilidades de clase)
- **Filtrado posterior**: Probabilidad > 30%, distancia < 50px, confianza > 80%

## Agradecimientos
Este proyecto estÃ¡ basado en los siguientes repositorios de cÃ³digo abierto, que facilitan enormemente nuestra investigaciÃ³n.

- Gracias a [DETR](https://github.com/facebookresearch/detr) por la implementaciÃ³n de [Detection Transformer](https://arxiv.org/abs/2005.12872)

- Gracias a [PRTR](https://github.com/mlpc-ucsd/PRTR/tree/main) por la implementaciÃ³n de [Pose Recognition with Cascade Transformers](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Pose_Recognition_With_Cascade_Transformers_CVPR_2021_paper.pdf)


## Herramientas Utilizadas

- ğŸ§° **[Git](https://git-scm.com/)**
- ğŸ—‚ **[GitHub](https://github.com/)**
- ğŸ **[Python](https://www.python.org/)**
- ğŸ”¥ **[Pytorch](https://pytorch.org/)**
- ğŸ³ **[Docker](https://www.docker.com/)**
- ğŸ“ˆ **[Streamlit](https://streamlit.io/)**
- ğŸ§¬ **[Hugging Face Transformers](https://huggingface.co/docs/transformers)**



## Autores

- **Jhonatan Steven Morales / jhonatan19991**  
- **Manuel Alejandro Perlaza / manuel-gruezo**  
- **Carol Dayana Varela / caroldvarela**  